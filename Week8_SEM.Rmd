---
title: "SEM & Lavaan"
author: "ZoÃ« Hawks"
date: "10/27/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r import, include = FALSE, message = FALSE, warning = FALSE}
#importing datasets
setwd("/Users/zashawks/Desktop/PNL/PKU/Verbal Fluency/Round 2 Analyses")
mydata <- read.csv("Master_data-new.csv", header = T)

setwd("/Users/zashawks/Desktop/Stats/Longitudinal/ALDA")
library(tidyverse)
library(tibble)
library(lubridate)
library(lavaan)
library(semPlot)
library(semTools)
library(dplyr)
library(tidyverse)
library(MASS)
library(psych)

#Converting group into factor variable (0 = Control, 1 = PKU)
mydata$GROUP <- as.factor(mydata$GROUP)

#Creating averages
mydata$Sem_TotalCorrect <- (mydata$Animal_TotalCorrect + mydata$Food_TotalCorrect)
mydata$Pho_TotalCorrect <- (mydata$F_TotalCorrect + mydata$S_TotalCorrect)

mydata$Sem_TotalUtterances <- (mydata$Animal_TotalUtterances + mydata$Food_TotalUtterances)
mydata$Pho_TotalUtterances <- (mydata$F_TotalUtterances + mydata$S_TotalUtterances)

mydata$Sem_Switches <- (mydata$Animal_Switches + mydata$Food_Switches)
mydata$Pho_Switches <- (mydata$F_Switches + mydata$S_Switches)

mydata$Sem_N_Clusters <- (mydata$Animal_N_Clusters + mydata$Food_N_Clusters)
mydata$Pho_N_Clusters <- (mydata$F_N_Clusters + mydata$S_N_Clusters)

mydata$Sem_TW_Clusters <- (mydata$Animal_TW_Clusters + mydata$Food_TW_Clusters)
mydata$Pho_TW_Clusters <- (mydata$F_TW_Clusters + mydata$S_TW_Clusters)

mydata$Animal_Mean_Clusters <- mydata$Animal_TW_Clusters/mydata$Animal_N_Clusters
mydata$Food_Mean_Clusters <- mydata$Food_TW_Clusters/mydata$Food_N_Clusters
mydata$F_Mean_Clusters <- mydata$F_TW_Clusters/mydata$F_N_Clusters
mydata$S_Mean_Clusters <- mydata$S_TW_Clusters/mydata$S_N_Clusters

mydata$Sem_Mean_Clusters <- (mydata$Animal_Mean_Clusters + mydata$Food_Mean_Clusters)
mydata$Pho_Mean_Clusters <- (mydata$F_Mean_Clusters + mydata$S_Mean_Clusters)

mydata$Sem_Errors <- (mydata$Animal_TotalUtterances + mydata$Food_TotalUtterances) -
  (mydata$Animal_TotalCorrect + mydata$Food_TotalCorrect)
mydata$Pho_Errors <- (mydata$F_TotalUtterances + mydata$S_TotalUtterances) -
  (mydata$F_TotalCorrect + mydata$S_TotalCorrect)

mydata$Sem_Singletons <- (mydata$Animal_TotalUtterances + mydata$Food_TotalUtterances) -
  (mydata$Animal_TW_Clusters + mydata$Food_TW_Clusters) - 
  (mydata$Animal_N_Clusters + mydata$Food_N_Clusters)
mydata$Pho_Singletons <- (mydata$F_TotalUtterances + mydata$S_TotalUtterances) -
  (mydata$F_TW_Clusters + mydata$S_TW_Clusters) -
  (mydata$F_N_Clusters + mydata$S_N_Clusters)
```

```{r, warning=FALSE, message=FALSE, include = FALSE}
##Raw score distributions & normalization
#Total Correct
hist(mydata$Sem_TotalCorrect)
qqnorm(mydata$Sem_TotalCorrect)
qqline(mydata$Sem_TotalCorrect)

hist(mydata$Pho_TotalCorrect)
qqnorm(mydata$Pho_TotalCorrect)
qqline(mydata$Pho_TotalCorrect)

psych::describe(mydata$Sem_TotalCorrect)
psych::describe(mydata$Pho_TotalCorrect)

#Phomenic data aren't normally distributed --> apply transformation
#Determining optimal lambda
trans1 <- boxcox(lm(mydata$Pho_TotalCorrect ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal lambda = .2626263
#Applying transform to data
mydata$Pho_TotalCorrect_BOX <- (mydata$Pho_TotalCorrect)^optimal_lambda1
describe(mydata$Pho_TotalCorrect_BOX)
hist(mydata$Pho_TotalCorrect_BOX)
qqnorm(mydata$Pho_TotalCorrect_BOX)
qqline(mydata$Pho_TotalCorrect_BOX)
mydata$Pho_TotalCorrect_BOX <- scale(mydata$Pho_TotalCorrect_BOX) #z-score Box-Cox transforms

##############################

#Switches
hist(mydata$Sem_Switches)
qqnorm(mydata$Sem_Switches)
qqline(mydata$Sem_Switches)

hist(mydata$Pho_Switches)
qqnorm(mydata$Pho_Switches)
qqline(mydata$Pho_Switches)

psych::describe(mydata$Sem_Switches)

#Phomenic data aren't normally distributed --> apply transformation
#Determining optimal lambda
trans1 <- boxcox(lm(mydata$Pho_Switches ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal labmda = .1010101
#Applying transform to data
mydata$Pho_Switches_BOX <- (mydata$Pho_Switches)^optimal_lambda1
describe(mydata$Pho_Switches_BOX)
hist(mydata$Pho_Switches_BOX)
qqnorm(mydata$Pho_Switches_BOX)
qqline(mydata$Pho_Switches_BOX)
mydata$Pho_Switches_BOX <- scale(mydata$Pho_Switches_BOX)

##########################

#N_Clusters
hist(mydata$Sem_N_Clusters)
qqnorm(mydata$Sem_N_Clusters)
qqline(mydata$Sem_N_Clusters)

hist(mydata$Pho_N_Clusters)
qqnorm(mydata$Pho_N_Clusters)
qqline(mydata$Pho_N_Clusters)

psych::describe(mydata$Sem_N_Clusters)
psych::describe(mydata$Pho_N_Clusters)

#Phomenic data aren't normally distributed --> apply transformation
#Determining optimal lambda
trans1 <- boxcox(lm(mydata$Pho_N_Clusters ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal lambda = .3434343
#Applying transform to data
mydata$Pho_N_Clusters_BOX <- (mydata$Pho_N_Clusters)^optimal_lambda1
psych::describe(mydata$Pho_N_Clusters_BOX)
hist(mydata$Pho_N_Clusters_BOX)
qqnorm(mydata$Pho_N_Clusters_BOX)
qqline(mydata$Pho_N_Clusters_BOX)
mydata$Pho_N_Clusters_BOX <- scale(mydata$Pho_N_Clusters_BOX)

#########################

#TotalUtterances
hist(mydata$Sem_TotalUtterances)
qqnorm(mydata$Sem_TotalUtterances)
qqline(mydata$Sem_TotalUtterances)

hist(mydata$Pho_TotalUtterances)
qqnorm(mydata$Pho_TotalUtterances)
qqline(mydata$Pho_TotalUtterances)

psych::describe(mydata$Sem_TotalUtterances)
psych::describe(mydata$Pho_TotalUtterances)

#Phomenic data aren't normally distributed --> apply transformation
#Determining optimal lambda
trans1 <- boxcox(lm(mydata$Pho_TotalUtterances ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #.06060606
#Applying transform to data
mydata$Pho_TotalUtterances_BOX <- (mydata$Pho_TotalUtterances)^optimal_lambda1
describe(mydata$Pho_TotalUtterances_BOX)
hist(mydata$Pho_TotalUtterances_BOX)
qqnorm(mydata$Pho_TotalUtterances_BOX)
qqline(mydata$Pho_TotalUtterances_BOX)
mydata$Pho_TotalUtterances_BOX <- scale(mydata$Pho_TotalUtterances_BOX)

#########################

#TW_Clusters
hist(mydata$Sem_TW_Clusters)
qqnorm(mydata$Sem_TW_Clusters)
qqline(mydata$Sem_TW_Clusters)

hist(mydata$Pho_TW_Clusters)
qqnorm(mydata$Pho_TW_Clusters)
qqline(mydata$Pho_TW_Clusters)

# psych::describe(mydata$Sem_TW_Clusters)
# mydata <- subset(mydata,!(mydata$Sem_TW_Clusters < 0))
# psych::describe(mydata$Sem_TW_Clusters)
# psych::describe(mydata$Pho_TW_Clusters)

#Phomenic data aren't normally distributed --> apply transformation
#Determining optimal lambda
trans1 <- boxcox(lm(mydata$Pho_TW_Clusters ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal lambda = .3030303
#Applying transform to data
mydata$Pho_TW_Clusters_BOX <- (mydata$Pho_TW_Clusters)^optimal_lambda1
describe(mydata$Pho_TW_Clusters_BOX)
hist(mydata$Pho_TW_Clusters_BOX)
qqnorm(mydata$Pho_TW_Clusters_BOX)
qqline(mydata$Pho_TW_Clusters_BOX)
mydata$Pho_TW_Clusters_BOX <- scale(mydata$Pho_TW_Clusters_BOX)

###############################

#ERRORS
#Determining optimal lambda
psych::describe(mydata$Pho_Errors)
mydata$Pho_Errors_Plus <- mydata$Pho_Errors + 1
trans1 <- boxcox(lm(mydata$Pho_Errors_Plus ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal labmda = .3434343
#Applying transform to data
mydata$Pho_Errors_BOX <- (mydata$Pho_Errors)^optimal_lambda1
psych::describe(mydata$Pho_Errors_BOX)
hist(mydata$Pho_Errors_BOX)
qqnorm(mydata$Pho_Errors_BOX)
qqline(mydata$Pho_Errors_BOX)
mydata$Pho_Errors_BOX <- scale(mydata$Pho_Errors_BOX)

##############################

#Singletons
#Determining optimal lambda
psych::describe(mydata$Pho_Singletons)
trans1 <- boxcox(lm(mydata$Pho_Singletons ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal labmda = .1414141
#Applying transform to data
mydata$Pho_Singletons_BOX <- (mydata$Pho_Singletons)^optimal_lambda1
psych::describe(mydata$Pho_Singletons_BOX)
hist(mydata$Pho_Singletons_BOX)
qqnorm(mydata$Pho_Singletons_BOX)
qqline(mydata$Pho_Singletons_BOX)
mydata$Pho_Singletons_BOX <- scale(mydata$Pho_Singletons_BOX)

##############################

#Mean_Clusters
#Determining optimal lambda
psych::describe(mydata$Pho_Mean_Clusters)
trans1 <- boxcox(lm(mydata$Pho_Mean_Clusters ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal labmda = -.9090909
#Applying transform to data
mydata$Pho_Mean_Clusters_BOX <- (mydata$Pho_Mean_Clusters)^optimal_lambda1
psych::describe(mydata$Pho_Mean_Clusters_BOX)
hist(mydata$Pho_Mean_Clusters_BOX)
qqnorm(mydata$Pho_Mean_Clusters_BOX)
qqline(mydata$Pho_Mean_Clusters_BOX)
mydata$Pho_Mean_Clusters_BOX <- scale(mydata$Pho_Mean_Clusters_BOX)

###################################

##Long to wide form
mydata_wide <- mydata %>% dplyr::select(-Age_at_timepoint_1) %>%
  gather(-c(ID2, Timepoint, GROUP), key = "variable", value = "value") %>%
  unite(varT, variable, Timepoint) %>%
  spread(key = varT, value = value)

mydata_wide$Animal_TW_Clusters_1 <- as.numeric(mydata_wide$Animal_TW_Clusters_1)
mydata_wide$Food_TW_Clusters_1 <- as.numeric(mydata_wide$Food_TW_Clusters_1)
mydata_wide$Food_Switches_1 <- as.numeric(mydata_wide$Food_Switches_1)
mydata_wide$Animal_Switches_1 <- as.numeric(mydata_wide$Animal_Switches_1)
mydata_wide$IQ_vocraw_1 <- as.numeric(mydata_wide$IQ_vocraw_1)
mydata_wide$IQ_mrraw_1 <- as.numeric(mydata_wide$IQ_mrraw_1)
mydata_wide$GROUP <- as.factor(mydata_wide$GROUP) 
mydata_wide$Age_at_time_of_testing_1 <- as.numeric(mydata_wide$Age_at_time_of_testing_1) 
#is this right to treat group as numeric? Summary doesn't provide AIC, BIC otherwise
```

## 1. Fit a measurement model to your constructs at one time point. Try out the different types of scaling discussed in class. What changes what stays the same?  
  
Measurement model was fit at timepoint 1. Under the marker method, parameter estimates were fixed at 1 for the first indicators (i.e., Animal_TW_Clusters_1 and Animal_Switches_1). Under the fixed factor method, in contrast, parameter estiamtes were fixed to 1 for the latent variables. In both cases, fit indices (e.g., logLikelihood, TLI, CFI, RMSEA) remain constant.   
```{r Problem 1, warning = F, message = F}
T1.mod <- ' 
            Semantic =~ Animal_TW_Clusters_1 + Food_TW_Clusters_1 + IQ_vocraw_1
            Phonemic =~ Animal_Switches_1 + Food_Switches_1 + IQ_mrraw_1
'

#Marker method
fit.marker <- cfa(T1.mod, data=mydata_wide)
summary(fit.marker, fit.measures = TRUE)
semPaths(fit.marker, layout = "tree", whatLabels = "est")
semPaths(fit.marker, layout = "tree", what = "std")

#Fixed factor method
fit.fixed <- cfa(T1.mod, data=mydata_wide, std.lv = T)
summary(fit.fixed, fit.measures = TRUE)
semPaths(fit.fixed, layout = "tree", whatLabels = "est")
semPaths(fit.fixed, layout = "tree", what = "std")
```

## 2. What do the fit statistics say about your latent variable? Good/bad? Is your latent variable Just identified/saturdated, under identified or over identified?  
  
* RMSEA = .165, SRMR = .091, TLI = .722, CFI = .852  
* RMSEA & SRMR > .08 and TLI & CFI < .90, suggesting poor fit -- that is, the latent variables are not effectively capturing commonalities among their indicator variables. This could be due to (1) high measurment error or (2) highly disparate indicators  
* This model is over identified, as evidenced by the positive degrees of freedom (15).  

## 3. Fit a longitudinal CFA model where you a) first correlate your latent factors across time and then b) a second model that predicts later times by a prevous time (ie auto regressive; t1 -> t2 -> t3). What are your conclusions? How does one differ from the other?  
  
For the longitudinal CFA model with correlated latent fators (Long.mod), I conclude that my latent factors are strongly correlated across time. Moreover, across all three timepoints, IQ verbal loads much more strongly onto the latent factors than Animal and Food total word counts, suggesting that IQ verbal may parsimonously account for shared variance among indicators.  

For the autoregressive CFA model (auto.mod), a similar picture emerges, suggesting that Semantic_1 is highly predictive of Semantic_2, which is highly predictive of Semantic_3. Of note, standardized variances for S_2 and S_3 latent factors are reduced for the autoregressive model compared to the correlated model (see path diagrams), likely due to the fact that S_2 and S_3 variances are accounted for by earlier timepoints. 
```{r, warning = F, message = F}
Long.mod <- ' 
            Semantic_1 =~ Animal_TW_Clusters_1 + Food_TW_Clusters_1 + IQ_vocraw_1
            Semantic_2 =~ Animal_TW_Clusters_2 + Food_TW_Clusters_2 + IQ_vocraw_2
            Semantic_3 =~ Animal_TW_Clusters_3 + Food_TW_Clusters_3 + IQ_vocraw_3
'

fit.long <- cfa(Long.mod, data=mydata_wide, std.lv = T)
summary(fit.long)
semPaths(fit.long, whatLabels = "std")
semPaths(fit.long, what = "std")

Auto.mod <- ' 
            Semantic_1 =~ Animal_TW_Clusters_1 + Food_TW_Clusters_1 + IQ_vocraw_1
            Semantic_2 =~ Animal_TW_Clusters_2 + Food_TW_Clusters_2 + IQ_vocraw_2
            Semantic_3 =~ Animal_TW_Clusters_3 + Food_TW_Clusters_3 + IQ_vocraw_3

            Semantic_3 ~ Semantic_2
            Semantic_2 ~ Semantic_1
'

fit.auto <- cfa(Auto.mod, data=mydata_wide, std.lv = T)
summary(fit.auto)
semPaths(fit.auto, layout = "circle2", whatLabels = "std")
semPaths(fit.auto, layout = "circle2", what = "std")

anova(fit.long, fit.auto) #simpler model (fit.long) is preferred 
```


## 4. Fit a longitdinal growth model in SEM and in HLM. Compare and contrast the differences.

## 5. Constrain the residual variances to be equal. Does this change the fit of your model?

## 6. Contrain your slope to be fixed, not random. How does this change your model?

## 7. Change the time metric in your SEM growth model. How does that change your estimates? Does it change your fit statistics?

## 8. Try a different type of estimation (see lavaan tutorial for details). How does that change your model?

## 9. Provide semplots for each of the models  
  
See above