---
title: "SEM & Lavaan"
author: "ZoÃ« Hawks"
date: "10/27/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r import, include = FALSE, message = FALSE, warning = FALSE}
#importing datasets
setwd("/Users/zashawks/Desktop/PNL/PKU/Verbal Fluency/Round 2 Analyses")
mydata <- read.csv("Master_data-new.csv", header = T)

setwd("/Users/zashawks/Desktop/Stats/Longitudinal/ALDA")
library(tidyverse)
library(tibble)
library(lubridate)
library(lavaan)
library(semPlot)
library(semTools)
library(dplyr)
library(tidyverse)
library(MASS)
library(psych)

#Converting group into factor variable (0 = Control, 1 = PKU)
mydata$GROUP <- as.factor(mydata$GROUP)

#Creating averages
mydata$Sem_TotalCorrect <- (mydata$Animal_TotalCorrect + mydata$Food_TotalCorrect)
mydata$Pho_TotalCorrect <- (mydata$F_TotalCorrect + mydata$S_TotalCorrect)

mydata$Sem_TotalUtterances <- (mydata$Animal_TotalUtterances + mydata$Food_TotalUtterances)
mydata$Pho_TotalUtterances <- (mydata$F_TotalUtterances + mydata$S_TotalUtterances)

mydata$Sem_Switches <- (mydata$Animal_Switches + mydata$Food_Switches)
mydata$Pho_Switches <- (mydata$F_Switches + mydata$S_Switches)

mydata$Sem_N_Clusters <- (mydata$Animal_N_Clusters + mydata$Food_N_Clusters)
mydata$Pho_N_Clusters <- (mydata$F_N_Clusters + mydata$S_N_Clusters)

mydata$Sem_TW_Clusters <- (mydata$Animal_TW_Clusters + mydata$Food_TW_Clusters)
mydata$Pho_TW_Clusters <- (mydata$F_TW_Clusters + mydata$S_TW_Clusters)

mydata$Animal_Mean_Clusters <- mydata$Animal_TW_Clusters/mydata$Animal_N_Clusters
mydata$Food_Mean_Clusters <- mydata$Food_TW_Clusters/mydata$Food_N_Clusters
mydata$F_Mean_Clusters <- mydata$F_TW_Clusters/mydata$F_N_Clusters
mydata$S_Mean_Clusters <- mydata$S_TW_Clusters/mydata$S_N_Clusters

mydata$Sem_Mean_Clusters <- (mydata$Animal_Mean_Clusters + mydata$Food_Mean_Clusters)
mydata$Pho_Mean_Clusters <- (mydata$F_Mean_Clusters + mydata$S_Mean_Clusters)

mydata$Sem_Errors <- (mydata$Animal_TotalUtterances + mydata$Food_TotalUtterances) -
  (mydata$Animal_TotalCorrect + mydata$Food_TotalCorrect)
mydata$Pho_Errors <- (mydata$F_TotalUtterances + mydata$S_TotalUtterances) -
  (mydata$F_TotalCorrect + mydata$S_TotalCorrect)

mydata$Sem_Singletons <- (mydata$Animal_TotalUtterances + mydata$Food_TotalUtterances) -
  (mydata$Animal_TW_Clusters + mydata$Food_TW_Clusters) - 
  (mydata$Animal_N_Clusters + mydata$Food_N_Clusters)
mydata$Pho_Singletons <- (mydata$F_TotalUtterances + mydata$S_TotalUtterances) -
  (mydata$F_TW_Clusters + mydata$S_TW_Clusters) -
  (mydata$F_N_Clusters + mydata$S_N_Clusters)
```

```{r, warning=FALSE, message=FALSE, include = FALSE}
##Raw score distributions & normalization
#Total Correct
hist(mydata$Sem_TotalCorrect)
qqnorm(mydata$Sem_TotalCorrect)
qqline(mydata$Sem_TotalCorrect)

hist(mydata$Pho_TotalCorrect)
qqnorm(mydata$Pho_TotalCorrect)
qqline(mydata$Pho_TotalCorrect)

psych::describe(mydata$Sem_TotalCorrect)
psych::describe(mydata$Pho_TotalCorrect)

#Phomenic data aren't normally distributed --> apply transformation
#Determining optimal lambda
trans1 <- boxcox(lm(mydata$Pho_TotalCorrect ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal lambda = .2626263
#Applying transform to data
mydata$Pho_TotalCorrect_BOX <- (mydata$Pho_TotalCorrect)^optimal_lambda1
describe(mydata$Pho_TotalCorrect_BOX)
hist(mydata$Pho_TotalCorrect_BOX)
qqnorm(mydata$Pho_TotalCorrect_BOX)
qqline(mydata$Pho_TotalCorrect_BOX)
mydata$Pho_TotalCorrect_BOX <- scale(mydata$Pho_TotalCorrect_BOX) #z-score Box-Cox transforms

##############################

#Switches
hist(mydata$Sem_Switches)
qqnorm(mydata$Sem_Switches)
qqline(mydata$Sem_Switches)

hist(mydata$Pho_Switches)
qqnorm(mydata$Pho_Switches)
qqline(mydata$Pho_Switches)

psych::describe(mydata$Sem_Switches)

#Phomenic data aren't normally distributed --> apply transformation
#Determining optimal lambda
trans1 <- boxcox(lm(mydata$Pho_Switches ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal labmda = .1010101
#Applying transform to data
mydata$Pho_Switches_BOX <- (mydata$Pho_Switches)^optimal_lambda1
describe(mydata$Pho_Switches_BOX)
hist(mydata$Pho_Switches_BOX)
qqnorm(mydata$Pho_Switches_BOX)
qqline(mydata$Pho_Switches_BOX)
mydata$Pho_Switches_BOX <- scale(mydata$Pho_Switches_BOX)

##########################

#N_Clusters
hist(mydata$Sem_N_Clusters)
qqnorm(mydata$Sem_N_Clusters)
qqline(mydata$Sem_N_Clusters)

hist(mydata$Pho_N_Clusters)
qqnorm(mydata$Pho_N_Clusters)
qqline(mydata$Pho_N_Clusters)

psych::describe(mydata$Sem_N_Clusters)
psych::describe(mydata$Pho_N_Clusters)

#Phomenic data aren't normally distributed --> apply transformation
#Determining optimal lambda
trans1 <- boxcox(lm(mydata$Pho_N_Clusters ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal lambda = .3434343
#Applying transform to data
mydata$Pho_N_Clusters_BOX <- (mydata$Pho_N_Clusters)^optimal_lambda1
psych::describe(mydata$Pho_N_Clusters_BOX)
hist(mydata$Pho_N_Clusters_BOX)
qqnorm(mydata$Pho_N_Clusters_BOX)
qqline(mydata$Pho_N_Clusters_BOX)
mydata$Pho_N_Clusters_BOX <- scale(mydata$Pho_N_Clusters_BOX)

#########################

#TotalUtterances
hist(mydata$Sem_TotalUtterances)
qqnorm(mydata$Sem_TotalUtterances)
qqline(mydata$Sem_TotalUtterances)

hist(mydata$Pho_TotalUtterances)
qqnorm(mydata$Pho_TotalUtterances)
qqline(mydata$Pho_TotalUtterances)

psych::describe(mydata$Sem_TotalUtterances)
psych::describe(mydata$Pho_TotalUtterances)

#Phomenic data aren't normally distributed --> apply transformation
#Determining optimal lambda
trans1 <- boxcox(lm(mydata$Pho_TotalUtterances ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #.06060606
#Applying transform to data
mydata$Pho_TotalUtterances_BOX <- (mydata$Pho_TotalUtterances)^optimal_lambda1
describe(mydata$Pho_TotalUtterances_BOX)
hist(mydata$Pho_TotalUtterances_BOX)
qqnorm(mydata$Pho_TotalUtterances_BOX)
qqline(mydata$Pho_TotalUtterances_BOX)
mydata$Pho_TotalUtterances_BOX <- scale(mydata$Pho_TotalUtterances_BOX)

#########################

#TW_Clusters
hist(mydata$Sem_TW_Clusters)
qqnorm(mydata$Sem_TW_Clusters)
qqline(mydata$Sem_TW_Clusters)

hist(mydata$Pho_TW_Clusters)
qqnorm(mydata$Pho_TW_Clusters)
qqline(mydata$Pho_TW_Clusters)

# psych::describe(mydata$Sem_TW_Clusters)
# mydata <- subset(mydata,!(mydata$Sem_TW_Clusters < 0))
# psych::describe(mydata$Sem_TW_Clusters)
# psych::describe(mydata$Pho_TW_Clusters)

#Phomenic data aren't normally distributed --> apply transformation
#Determining optimal lambda
trans1 <- boxcox(lm(mydata$Pho_TW_Clusters ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal lambda = .3030303
#Applying transform to data
mydata$Pho_TW_Clusters_BOX <- (mydata$Pho_TW_Clusters)^optimal_lambda1
describe(mydata$Pho_TW_Clusters_BOX)
hist(mydata$Pho_TW_Clusters_BOX)
qqnorm(mydata$Pho_TW_Clusters_BOX)
qqline(mydata$Pho_TW_Clusters_BOX)
mydata$Pho_TW_Clusters_BOX <- scale(mydata$Pho_TW_Clusters_BOX)

###############################

#ERRORS
#Determining optimal lambda
psych::describe(mydata$Pho_Errors)
mydata$Pho_Errors_Plus <- mydata$Pho_Errors + 1
trans1 <- boxcox(lm(mydata$Pho_Errors_Plus ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal labmda = .3434343
#Applying transform to data
mydata$Pho_Errors_BOX <- (mydata$Pho_Errors)^optimal_lambda1
psych::describe(mydata$Pho_Errors_BOX)
hist(mydata$Pho_Errors_BOX)
qqnorm(mydata$Pho_Errors_BOX)
qqline(mydata$Pho_Errors_BOX)
mydata$Pho_Errors_BOX <- scale(mydata$Pho_Errors_BOX)

##############################

#Singletons
#Determining optimal lambda
psych::describe(mydata$Pho_Singletons)
trans1 <- boxcox(lm(mydata$Pho_Singletons ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal labmda = .1414141
#Applying transform to data
mydata$Pho_Singletons_BOX <- (mydata$Pho_Singletons)^optimal_lambda1
psych::describe(mydata$Pho_Singletons_BOX)
hist(mydata$Pho_Singletons_BOX)
qqnorm(mydata$Pho_Singletons_BOX)
qqline(mydata$Pho_Singletons_BOX)
mydata$Pho_Singletons_BOX <- scale(mydata$Pho_Singletons_BOX)

##############################

#Mean_Clusters
#Determining optimal lambda
psych::describe(mydata$Pho_Mean_Clusters)
trans1 <- boxcox(lm(mydata$Pho_Mean_Clusters ~ 1), plotit = T)
range(trans1$x[trans1$y > max(trans1$y)-qchisq(0.95,1)/2]) #95% lambda range
trans_df = as.data.frame(trans1) #optimal lambda
optimal_lambda1 = trans_df[which.max(trans1$y),1]
optimal_lambda1 #optimal labmda = -.9090909
#Applying transform to data
mydata$Pho_Mean_Clusters_BOX <- (mydata$Pho_Mean_Clusters)^optimal_lambda1
psych::describe(mydata$Pho_Mean_Clusters_BOX)
hist(mydata$Pho_Mean_Clusters_BOX)
qqnorm(mydata$Pho_Mean_Clusters_BOX)
qqline(mydata$Pho_Mean_Clusters_BOX)
mydata$Pho_Mean_Clusters_BOX <- scale(mydata$Pho_Mean_Clusters_BOX)

###################################

##Long to wide form
mydata_wide <- mydata %>% dplyr::select(-Age_at_timepoint_1) %>%
  gather(-c(ID2, Timepoint, GROUP), key = "variable", value = "value") %>%
  unite(varT, variable, Timepoint) %>%
  spread(key = varT, value = value)

mydata_wide$Animal_TW_Clusters_1 <- as.numeric(mydata_wide$Animal_TW_Clusters_1)
mydata_wide$Food_TW_Clusters_1 <- as.numeric(mydata_wide$Food_TW_Clusters_1)
mydata_wide$Food_Switches_1 <- as.numeric(mydata_wide$Food_Switches_1)
mydata_wide$Animal_Switches_1 <- as.numeric(mydata_wide$Animal_Switches_1)
mydata_wide$IQ_vocraw_1 <- as.numeric(mydata_wide$IQ_vocraw_1)
mydata_wide$IQ_mrraw_1 <- as.numeric(mydata_wide$IQ_mrraw_1)
mydata_wide$GROUP <- as.factor(mydata_wide$GROUP) 
mydata_wide$Age_at_time_of_testing_1 <- as.numeric(mydata_wide$Age_at_time_of_testing_1) 
#is this right to treat group as numeric? Summary doesn't provide AIC, BIC otherwise
```

## 1. Fit a measurement model to your constructs at one time point. Try out the different types of scaling discussed in class. What changes what stays the same?  
  
Measurement model was fit at timepoint 1. Under the marker method, parameter estimates were fixed at 1 for the first indicators (i.e., Animal_TW_Clusters_1 and Animal_Switches_1). Under the fixed factor method, in contrast, parameter estiamtes were fixed to 1 for the latent variables. In both cases, fit indices (e.g., logLikelihood, TLI, CFI, RMSEA) remain constant.   
```{r Problem 1, warning = F, message = F}
T1.mod <- ' 
            Semantic =~ Animal_TW_Clusters_1 + Food_TW_Clusters_1 + IQ_vocraw_1
            Phonemic =~ Animal_Switches_1 + Food_Switches_1 + IQ_mrraw_1
'

#Marker method
fit.marker <- cfa(T1.mod, data=mydata_wide)
summary(fit.marker, fit.measures = TRUE)
semPaths(fit.marker, layout = "tree", whatLabels = "est")
semPaths(fit.marker, layout = "tree", what = "std")

#Fixed factor method
fit.fixed <- cfa(T1.mod, data=mydata_wide, std.lv = T)
summary(fit.fixed, fit.measures = TRUE)
semPaths(fit.fixed, layout = "tree", whatLabels = "est")
semPaths(fit.fixed, layout = "tree", what = "std")
```

## 2. What do the fit statistics say about your latent variable? Good/bad? Is your latent variable Just identified/saturdated, under identified or over identified?  
  
* RMSEA = .165, SRMR = .091, TLI = .722, CFI = .852  
* RMSEA & SRMR > .08 and TLI & CFI < .90, suggesting poor fit -- that is, the latent variables are not effectively capturing commonalities among their indicator variables. This could be due to (1) high measurment error or (2) highly disparate indicators  
* This model is over identified, as evidenced by the positive degrees of freedom (15).  

## 3. Fit a longitudinal CFA model where you a) first correlate your latent factors across time and then b) a second model that predicts later times by a prevous time (ie auto regressive; t1 -> t2 -> t3). What are your conclusions? How does one differ from the other?  
  
For the longitudinal CFA model with correlated latent fators (Long.mod), I conclude that my latent factors are strongly correlated across time. Moreover, across all three timepoints, IQ verbal loads much more strongly onto the latent factors than Animal and Food total word counts, suggesting that IQ verbal may parsimonously account for shared variance among indicators.  

For the autoregressive CFA model (auto.mod), a similar picture emerges, suggesting that Semantic_1 is highly predictive of Semantic_2, which is highly predictive of Semantic_3. Of note, standardized variances for S_2 and S_3 latent factors are reduced for the autoregressive model compared to the correlated model (see path diagrams), likely due to the fact that S_2 and S_3 variances are accounted for by earlier timepoints. 
```{r, warning = F, message = F}
Long.mod <- ' 
            Semantic_1 =~ Animal_TW_Clusters_1 + Food_TW_Clusters_1 + IQ_vocraw_1
            Semantic_2 =~ Animal_TW_Clusters_2 + Food_TW_Clusters_2 + IQ_vocraw_2
            Semantic_3 =~ Animal_TW_Clusters_3 + Food_TW_Clusters_3 + IQ_vocraw_3
'

fit.long <- cfa(Long.mod, data=mydata_wide, std.lv = T)
summary(fit.long)
semPaths(fit.long, whatLabels = "std")
semPaths(fit.long, what = "std")

Auto.mod <- ' 
            Semantic_1 =~ Animal_TW_Clusters_1 + Food_TW_Clusters_1 + IQ_vocraw_1
            Semantic_2 =~ Animal_TW_Clusters_2 + Food_TW_Clusters_2 + IQ_vocraw_2
            Semantic_3 =~ Animal_TW_Clusters_3 + Food_TW_Clusters_3 + IQ_vocraw_3

            Semantic_3 ~ Semantic_2
            Semantic_2 ~ Semantic_1
'

fit.auto <- cfa(Auto.mod, data=mydata_wide, std.lv = T)
summary(fit.auto)
semPaths(fit.auto, layout = "circle2", whatLabels = "std")
semPaths(fit.auto, layout = "circle2", what = "std")

anova(fit.long, fit.auto) #simpler model (fit.long) is preferred 
```


## 4. Fit a longitdinal growth model in SEM and in HLM. Compare and contrast the differences.  

Estimates of *intercept* are similar (1) between fixed slope SEM & HLM models, (2) between random slope SEM & HLM models, and (3) between fixed-slope + covariate SEM & HLM models. Estimates of *slope* are similar (1) between fixed slope SEM & HLM models and (2) between random slope SEM & HLM models. Esimates of slope differ between SEM & HLM models when a covariate is added because the HLM slope can no longer be interpreted as the straight-forward rate of increase (y over x) between timepoints. As well, logLikelihood tests designated the fixed slope + covariate model as the preferred model regardless of whether SEM or HLM was used.
```{r, warning = FALSE, message = FALSE}
#HLM model
library(lme4)
library(car)
mod.HLM <- lmer(Sem_TotalCorrect ~ Timepoint + (1 | ID2), data = mydata) #fixed slope
summary(mod.HLM) #intercept = 31.06, slope = 3.85
car::Anova(mod.HLM)

mod.HLM2 <- lmer(Sem_TotalCorrect ~ Timepoint + (Timepoint | ID2), data = mydata) #random slope
summary(mod.HLM2) #intercept = 31.07, slope = 3.84
car::Anova(mod.HLM2)

mod.HLM3 <- lmer(Sem_TotalCorrect ~ Timepoint + Age_at_time_of_testing + (1 | ID2),
                 data = mydata) #fixed slope, with covariate
summary(mod.HLM3) #intercept = 14.83, est. timepoint = 1.83, est. age = 1.55
car::Anova(mod.HLM3)

anova(mod.HLM, mod.HLM2, mod.HLM3) #fixed slope, covariate model is preferred

#Growth model
mod.SEM <- ' intercept =~ 1*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 1*Sem_TotalCorrect_3
             slope =~ 0*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 2*Sem_TotalCorrect_3
             slope ~~ 0*slope ' #fixed slope, no variance

mod.SEM.fixed <- growth(mod.SEM, missing = "ML", data = mydata_wide)
summary(mod.SEM.fixed) #intercept = 34.93, slope = 3.85
semPaths(mod.SEM.fixed)
semPaths(mod.SEM.fixed, what = "std")

mod.SEM2 <- ' intercept =~ 1*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 1*Sem_TotalCorrect_3
              slope =~ 0*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 2*Sem_TotalCorrect_3 ' #random slope model

mod.SEM.random <- growth(mod.SEM2, missing = "ML", data = mydata_wide)
summary(mod.SEM.random) #intercept = 34.93, slope = 3.85
semPaths(mod.SEM.random)
semPaths(mod.SEM.random, what = "std")

mod.SEM3 <- ' intercept =~ 1*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 1*Sem_TotalCorrect_3
              slope =~ 0*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 2*Sem_TotalCorrect_3
              slope ~~ 0*slope

              Sem_TotalCorrect_1 ~ Age_at_time_of_testing_1
              Sem_TotalCorrect_2 ~ Age_at_time_of_testing_2
              Sem_TotalCorrect_3 ~ Age_at_time_of_testing_3 ' #fixed slope, with covariate

mod.SEM.cov <- growth(mod.SEM3, missing = "ML", data = mydata_wide)
summary(mod.SEM.cov) #intercept = 12.38, slope = 6.65
semPaths(mod.SEM.cov)
semPaths(mod.SEM.cov, what = "std")

anova(mod.SEM.fixed, mod.SEM.random, mod.SEM.cov) #fixed slope, covariate model is preferred
```

## 5. Constrain the residual variances to be equal. Does this change the fit of your model?  

Constraining the residual variances does not significantly change model fit. LogLikelihood tests indicate that a simpler model, where residual variances are allowed to vary, is preferred to a more complex model where they are constrained to be equal.
```{r}
mod.SEM4 <- ' intercept =~ 1*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 1*Sem_TotalCorrect_3
              slope =~ 0*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 2*Sem_TotalCorrect_3
              
              Sem_TotalCorrect_1 ~ Age_at_time_of_testing_1
              Sem_TotalCorrect_2 ~ Age_at_time_of_testing_2
              Sem_TotalCorrect_3 ~ Age_at_time_of_testing_3

              Sem_TotalCorrect_1 ~~ a*Sem_TotalCorrect_1
              Sem_TotalCorrect_2 ~~ a*Sem_TotalCorrect_2
              Sem_TotalCorrect_3 ~~ a*Sem_TotalCorrect_3 ' #random slope, with covariate, residual variances are equal

mod.SEM.cov2 <- growth(mod.SEM4, missing = "ML", data = mydata_wide)
summary(mod.SEM.cov2) #intercept = 12.53, slope = 6.23
semPaths(mod.SEM.cov2)
semPaths(mod.SEM.cov2, what = "std")

anova(mod.SEM.cov, mod.SEM.cov2) #simpler model (mod.SEM.cov) is preferred
```

## 6. Contrain your slope to be fixed, not random. How does this change your model?  

Constraining slopes to be fixed does not sigificantly change my model (p = .97).
```{r}
anova(mod.SEM.fixed, mod.SEM.random) #see problem #4 for model specifics
```

## 7. Change the time metric in your SEM growth model. How does that change your estimates? Does it change your fit statistics?  

I changed my time metric such that the intercept was centered at TP 3 rather than TP1. This increased the intercept (which makes sense, given age-related change) but did not change model fit.  
  
I also changed my time metric such that "duration" between TP3 & TP2 > "duration" between TP2 & TP1. This had little impact on the intercept, but decreased the slope and improved model fit.
```{r, warning=FALSE, message = FALSE}
# mydata_wide$time_1 <- 0
# mydata_wide$time_2 <- as.numeric(mydata_wide$Age_at_time_of_testing_2) -
#   as.numeric(mydata_wide$Age_at_time_of_testing_1) 
# mydata_wide$time_3 <- as.numeric(mydata_wide$Age_at_time_of_testing_3) - 
#   as.numeric(mydata_wide$Age_at_time_of_testing_1) 

mod.SEM5 <- ' intercept =~ 1*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 1*Sem_TotalCorrect_3
              slope =~ -2*Sem_TotalCorrect_1 + -1*Sem_TotalCorrect_2 + 0*Sem_TotalCorrect_3 '

mod.SEM.time <- growth(mod.SEM5, missing = "ML", data = mydata_wide)
summary(mod.SEM.time) 
semPaths(mod.SEM.time)
semPaths(mod.SEM.time, what = "std")

anova(mod.SEM.random, mod.SEM.time)

mod.SEM6 <- ' intercept =~ 1*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 1*Sem_TotalCorrect_3
              slope =~ 0*Sem_TotalCorrect_1 + 1*Sem_TotalCorrect_2 + 6*Sem_TotalCorrect_3 '

mod.SEM.time2 <- growth(mod.SEM6, missing = "ML", data = mydata_wide)
summary(mod.SEM.time2) 
semPaths(mod.SEM.time2)
semPaths(mod.SEM.time2, what = "std")

anova(mod.SEM.random, mod.SEM.time2)
```

## 8. Try a different type of estimation (see lavaan tutorial for details). How does that change your model?  

Despite the fact that Pho_TotalCorrect is non-normal, changing the esimator has no effect on model fit, intercept, or slope.
```{r}
#What if we want to try to model non-normal data?
hist(mydata$Pho_TotalCorrect)

mod.SEM7 <- ' intercept =~ 1*Pho_TotalCorrect_1 + 1*Pho_TotalCorrect_2 + 1*Pho_TotalCorrect_3
              slope =~ -2*Pho_TotalCorrect_1 + -1*Pho_TotalCorrect_2 + 0*Pho_TotalCorrect_3 '

mod.SEM.MLM <- growth(mod.SEM7, estimator = "MLM", data = mydata_wide)
mod.SEM.ML <- growth(mod.SEM7, estimator = "ML", data = mydata_wide)

summary(mod.SEM.MLM) 
semPaths(mod.SEM.MLM)
semPaths(mod.SEM.MLM, what = "std")

summary(mod.SEM.ML) 
semPaths(mod.SEM.ML)
semPaths(mod.SEM.ML, what = "std")

anova(mod.SEM.MLM, mod.SEM.ML)
```

## 9. Provide semplots for each of the models  
  
Incorporated throughout code (above)